{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPciPQiKxRgH9mwkR3Eq1A8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"_Aw8LxoFriVv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749821901568,"user_tz":-120,"elapsed":57309,"user":{"displayName":"Daphn√© Baudeu","userId":"10137981154069023525"}},"outputId":"7d720093-f1af-4dc7-a19e-eec2078fff7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","Attempting to access data in: /content/gdrive/MyDrive/DnARnAProject/data/\n","--- Inspecting data.npz ---\n","Keys in data.npz: ['sequence', 'expressed_plus', 'expressed_minus']\n","Sequence array shape: (12157105,)\n","Sequence array dtype: uint8\n","First 100 sequence values: [1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1\n"," 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 3 1 1 3 0 0 1 0 1 3 0 1\n"," 1 1 3 0 0 1 0 1 0 2 1 1 1 3 0 0 3 1 3 0 0 1 1 1 3 2]\n","Unique values in sequence (sample): [0 1 2 3]\n","Base counts in entire sequence array:\n","{np.uint8(1): 2320576, np.uint8(0): 3766349, np.uint8(3): 3753080, np.uint8(2): 2317100}\n","Unique values in first 1000 bases: [0 1 2 3]\n","Unique values in first 10000 bases: [0 1 2 3]\n","Expression_plus array shape: (12157105,)\n","Expression_plus array dtype: uint8\n","First 10 expression_plus values: [0 0 0 0 0 0 0 0 0 0]\n","Unique values in expression_plus (sample): [0 1]\n","Expression_minus array shape: (12157105,)\n","Expression_minus array dtype: uint8\n","First 10 expression_minus values: [0 0 0 0 0 0 0 0 0 0]\n","Unique values in expression_minus (sample): [0 1]\n","\n","--- Inspecting regions.parquet ---\n","Regions DataFrame shape: (15705825, 6)\n","Regions DataFrame head:\n","  contig strand  start  offset  window_size  num_expressed\n","0   chrI      +   7655    7655         2048            205\n","1   chrI      +   7656    7656         2048            206\n","2   chrI      +   7657    7657         2048            207\n","3   chrI      +   7658    7658         2048            208\n","4   chrI      +   7659    7659         2048            209\n","\n","Regions DataFrame info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 15705825 entries, 0 to 15705824\n","Data columns (total 6 columns):\n"," #   Column         Dtype \n","---  ------         ----- \n"," 0   contig         object\n"," 1   strand         object\n"," 2   start          int64 \n"," 3   offset         int64 \n"," 4   window_size    int64 \n"," 5   num_expressed  int64 \n","dtypes: int64(4), object(2)\n","memory usage: 719.0+ MB\n","\n","Unique strands: ['+' '-']\n","\n","--- Inspecting ensembl_annotation.gff3 (Optional) ---\n","First 10 lines of GFF3 file:\n","##gff-version 3\n","##sequence-region   I 1 230218\n","##sequence-region   II 1 813184\n","##sequence-region   III 1 316620\n","##sequence-region   IV 1 1531933\n","##sequence-region   IX 1 439888\n","##sequence-region   Mito 1 85779\n","##sequence-region   V 1 576874\n","##sequence-region   VI 1 270161\n","##sequence-region   VII 1 1090940\n","\n","--- Instantiating and Testing GenomeExpressionDataset ---\n","Successfully instantiated GenomeExpressionDataset.\n","Total number of samples (regions) in the dataset: 15705825\n","\n","--- Accessing individual samples ---\n","Sample at index 0:\n","  Sequence shape: torch.Size([2048, 5]) (One-hot encoded)\n","  Label: 0\n","  Metadata: {'contig': 'chrI', 'strand': '+', 'start': 7655, 'offset': 7655, 'window_size': 2048, 'num_expressed': 205}\n","\n","Sample at index 500:\n","  Sequence shape: torch.Size([2048, 5]) (One-hot encoded)\n","  Label: 0\n","  Metadata: {'contig': 'chrI', 'strand': '+', 'start': 8155, 'offset': 8155, 'window_size': 2048, 'num_expressed': 251}\n","\n","--- Using DataLoader to get batches ---\n","\n","--- Batch 1 ---\n","  Batch of sequences shape: torch.Size([8, 2048, 5])\n","  Batch of labels shape: torch.Size([8])\n","  Labels in this batch: [0 1 0 1 1 1 1 1]\n","  Metadata for first sample in batch:\n","    Contig: chrIV, Strand: +, Offset: 1962684\n","\n","--- Batch 2 ---\n","  Batch of sequences shape: torch.Size([8, 2048, 5])\n","  Batch of labels shape: torch.Size([8])\n","  Labels in this batch: [1 0 1 0 1 1 0 0]\n","  Metadata for first sample in batch:\n","    Contig: chrV, Strand: +, Offset: 3660269\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","from collections import Counter\n","from torch.utils.data import Dataset, DataLoader\n","# from Bio import SeqIO # Uncomment if you want to inspect GFF3 files (e.g., ensembl_annotation.gff3) later with Biopython\n","\n","# --- STEP 1: Mount Google Drive ---\n","# This is the initial step to make your Google Drive files accessible within Google Colab.\n","# When you run this cell, a pop-up window will appear asking you to authorize Colab\n","# to access your Google Drive. You'll need to select your Google account and grant permissions.\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# --- Configuration ---\n","# This section defines the paths to your data files within your Google Drive.\n","# It's crucial to set these paths correctly so your code can find the data.\n","\n","# `google_drive_project_path` points to the folder in your Google Drive\n","# where your main project files (including the 'data' subfolder) are located.\n","# Based on your previous information, your data is in 'My Drive/DnARnAProject/data'.\n","# So, the 'DnARnAProject' folder is the direct parent of your 'data' folder.\n","google_drive_project_path = '/content/gdrive/MyDrive/DnARnAProject/'\n","\n","# `data_dir` constructs the full path to your 'data' folder by joining the project path\n","# with the 'data' folder name. This is where data.npz, regions.parquet, etc., are stored.\n","data_dir = os.path.join(google_drive_project_path, 'data/')\n","\n","print(f\"Attempting to access data in: {data_dir}\")\n","\n","# --- Verify the data directory exists ---\n","# This block checks if the specified data directory actually exists in your mounted Google Drive.\n","# It's a critical debugging step to ensure your path is correct before attempting to load data.\n","if not os.path.isdir(data_dir):\n","    print(f\"Error: The directory '{data_dir}' does not exist.\")\n","    print(\"Please check your Google Drive path and folder names for typos.\")\n","    print(\"Listing contents of your project folder for debugging:\")\n","    try:\n","        # This helps in debugging by showing what's actually inside the assumed project path.\n","        # It can help you spot if 'DnARnAProject' is misspelled or located elsewhere.\n","        print(os.listdir(google_drive_project_path))\n","    except FileNotFoundError:\n","        # Handles the case where even the parent project folder isn't found.\n","        print(f\"Project folder '{google_drive_project_path}' not found either. Check the full path.\")\n","    # Note: We don't use exit() here in a Colab notebook to avoid stopping the entire runtime.\n","    # However, subsequent cells relying on `data_dir` will likely fail if the path is wrong.\n","\n","\n","# --- 1. Inspect data.npz ---\n","# This section attempts to load and inspect 'data.npz'.\n","# This file contains the concatenated sequence and expression data for all chromosomes.\n","print(\"--- Inspecting data.npz ---\")\n","try:\n","    data_npz_path = os.path.join(data_dir, 'data.npz')\n","    # np.load() is used to load .npz files, which are zipped NumPy arrays.\n","    data_npz = np.load(data_npz_path)\n","\n","    print(f\"Keys in data.npz: {list(data_npz.keys())}\") # Shows what arrays are stored inside the .npz file.\n","\n","    # Check for 'sequence' data, its shape, data type, and a sample of values.\n","    if 'sequence' in data_npz:\n","        seq_array = data_npz['sequence']\n","        print(f\"Sequence array shape: {seq_array.shape}\") # (num_bases_total,) indicating a 1D array of base encodings.\n","        print(f\"Sequence array dtype: {seq_array.dtype}\") # Typically uint8 for integer encodings (A=0, C=1, G=2, T=3, N=4).\n","        print(f\"First 100 sequence values: {seq_array[:100]}\")\n","        # Sample unique values to confirm the encoding scheme (e.g., 0, 1, 2, 3, 4).\n","        unique_seq_values = np.unique(seq_array[:10000])\n","        print(f\"Unique values in sequence (sample): {unique_seq_values}\")\n","        counts = dict(Counter(seq_array))\n","        print(\"Base counts in entire sequence array:\")\n","        print(counts)\n","        print(\"Unique values in first 1000 bases:\", np.unique(seq_array[:1000]))\n","        print(\"Unique values in first 10000 bases:\", np.unique(seq_array[:10000]))\n","\n","\n","    # Check for 'expression_plus' data (forward strand expression).\n","    if 'expressed_plus' in data_npz:\n","        expr_plus_array = data_npz['expressed_plus']\n","        print(f\"Expression_plus array shape: {expr_plus_array.shape}\") # (num_bases_total,) similar to sequence.\n","        print(f\"Expression_plus array dtype: {expr_plus_array.dtype}\") # Typically uint8 (expressed=1, unexpressed=0).\n","        print(f\"First 10 expression_plus values: {expr_plus_array[:10]}\")\n","        unique_expr_values = np.unique(expr_plus_array[:10000])\n","        print(f\"Unique values in expression_plus (sample): {unique_expr_values}\")\n","\n","    # Check for 'expression_minus' data (backward strand expression).\n","    if 'expressed_minus' in data_npz:\n","        expr_minus_array = data_npz['expressed_minus']\n","        print(f\"Expression_minus array shape: {expr_minus_array.shape}\")\n","        print(f\"Expression_minus array dtype: {expr_minus_array.dtype}\")\n","        print(f\"First 10 expression_minus values: {expr_minus_array[:10]}\")\n","        unique_expr_values = np.unique(expr_minus_array[:10000])\n","        print(f\"Unique values in expression_minus (sample): {unique_expr_values}\")\n","\n","except FileNotFoundError:\n","    print(f\"Error: {data_npz_path} not found. This typically means the path is incorrect after mounting Drive.\")\n","    print(\"Double-check the folder names in your Google Drive for typos.\")\n","except Exception as e:\n","    print(f\"An error occurred while loading data.npz: {e}\")\n","\n","# --- 2. Inspect regions.parquet ---\n","# This section loads and inspects 'regions.parquet'.\n","# This file contains suggested training regions with their offsets and window sizes.\n","print(\"\\n--- Inspecting regions.parquet ---\")\n","try:\n","    regions_parquet_path = os.path.join(data_dir, 'regions.parquet')\n","    # pd.read_parquet() is used to load data from Parquet files into a Pandas DataFrame.\n","    regions_df = pd.read_parquet(regions_parquet_path)\n","\n","    print(f\"Regions DataFrame shape: {regions_df.shape}\") # Displays the number of rows (regions) and columns.\n","    print(\"Regions DataFrame head:\")\n","    print(regions_df.head()) # Shows the first few rows of the DataFrame, providing a quick look at the data.\n","    print(\"\\nRegions DataFrame info:\")\n","    regions_df.info() # Provides a summary of the DataFrame, including data types and non-null values for each column.\n","\n","    print(f\"\\nUnique strands: {regions_df['strand'].unique()}\") # Checks for unique values in the 'strand' column (e.g., '+' or '-').\n","\n","except FileNotFoundError:\n","    print(f\"Error: {regions_parquet_path} not found. This typically means the path is incorrect after mounting Drive.\")\n","except Exception as e:\n","    print(f\"An error occurred while loading regions.parquet: {e}\")\n","\n","\n","# --- 3. Inspect ensembl_annotation.gff3 (Optional) ---\n","# This section attempts to inspect 'ensembl_annotation.gff3'.\n","# This is an annotation file from Ensembl and is optional for the DataLoader itself,\n","# but can be useful for understanding the problem.\n","print(\"\\n--- Inspecting ensembl_annotation.gff3 (Optional) ---\")\n","try:\n","    gff3_path = os.path.join(data_dir, 'ensembl_annotation.gff3')\n","    # This block opens the GFF3 file and prints its first 10 lines to give a glimpse of its format.\n","    with open(gff3_path, \"r\") as handle:\n","        gff_lines = [next(handle) for _ in range(10)]\n","        print(\"First 10 lines of GFF3 file:\")\n","        for line in gff_lines:\n","            print(line.strip()) # .strip() removes leading/trailing whitespace including newlines.\n","\n","except FileNotFoundError:\n","    print(f\"Error: {gff3_path} not found. This file is optional for the DataLoader.\")\n","except Exception as e:\n","    print(f\"An error occurred while inspecting ensembl_annotation.gff3: {e}\")\n","\n","\n","# --- Custom PyTorch DataLoader Class (same as before) ---\n","# This is your core data loading class for PyTorch, inheriting from torch.utils.data.Dataset.\n","# It defines how individual data samples (sequence segments and expression labels) are loaded.\n","class GenomeExpressionDataset(Dataset):\n","    \"\"\"\n","    Custom Dataset for loading DNA sequence and expression data for genomic regions.\n","    It loads data from pre-processed .npz and .parquet files.\n","    \"\"\"\n","    def __init__(self, data_dir):\n","        \"\"\"\n","        Initializes the dataset by loading the full sequence and expression arrays\n","        and the DataFrame of genomic regions.\n","\n","        Args:\n","            data_dir (str): The path to the directory containing 'data.npz' and 'regions.parquet'.\n","        \"\"\"\n","        self.data_dir = data_dir\n","\n","        # Construct full paths to data files\n","        self.data_npz_path = os.path.join(data_dir, 'data.npz')\n","        self.regions_parquet_path = os.path.join(data_dir, 'regions.parquet')\n","\n","        # Load data.npz containing sequence and expression arrays\n","        try:\n","            self.data_npz = np.load(self.data_npz_path)\n","            self.sequence_data = self.data_npz['sequence'] # Array of encoded DNA bases (0-4)\n","            # IMPORTANT: Corrected key to 'expressed_plus' based on your data.npz keys\n","            self.expression_plus_data = self.data_npz['expressed_plus'] # Array of expression labels for forward strand (0 or 1)\n","            # IMPORTANT: Corrected key to 'expressed_minus' based on your data.npz keys\n","            self.expression_minus_data = self.data_npz['expressed_minus'] # Array of expression labels for reverse strand (0 or 1)\n","        except Exception as e:\n","            raise RuntimeError(f\"Could not load data from {self.data_npz_path}. Make sure the file exists and is not corrupted: {e}\")\n","\n","        # Load regions.parquet containing metadata for each genomic region\n","        try:\n","            self.regions_df = pd.read_parquet(self.regions_parquet_path)\n","        except Exception as e:\n","            raise RuntimeError(f\"Could not load regions from {self.regions_parquet_path}. Make sure the file exists and is not corrupted: {e}\")\n","\n","        # Number of unique nucleotides/channels for one-hot encoding (A, C, G, T, N)\n","        self.num_nucleotides = 5\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the total number of samples (genomic regions) in the dataset.\n","        This is determined by the number of rows in the regions DataFrame.\n","        \"\"\"\n","        return len(self.regions_df)\n","\n","    def _one_hot_encode(self, sequence_segment):\n","        \"\"\"\n","        Converts a sequence segment (array of integer encodings) into a one-hot encoded tensor.\n","        Example: [0, 1, 2] (A, C, G) -> [[1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0]]\n","        \"\"\"\n","        one_hot_tensor = torch.zeros(len(sequence_segment), self.num_nucleotides, dtype=torch.float32)\n","        one_hot_tensor.scatter_(1, torch.tensor(sequence_segment).unsqueeze(1).long(), 1)\n","        return one_hot_tensor\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieves a single data sample (sequence, expression label, and metadata) by its index.\n","\n","        Args:\n","            idx (int or torch.Tensor): The index of the region to retrieve.\n","\n","        Returns:\n","            tuple: A tuple containing:\n","                - encoded_sequence (torch.Tensor): The one-hot encoded DNA sequence segment.\n","                - expression_label (torch.Tensor): The expression label (0 or 1) for the region.\n","                - region_info (dict): A dictionary containing metadata for the region (e.g., contig, strand, offset).\n","        \"\"\"\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        region_info = self.regions_df.iloc[idx]\n","\n","        offset = region_info['offset'] #\n","        window_size = region_info['window_size'] #\n","        strand = region_info['strand']\n","\n","        sequence_segment = self.sequence_data[offset : offset + window_size]\n","        encoded_sequence = self._one_hot_encode(sequence_segment)\n","\n","        if strand == '+':\n","            expression_label = self.expression_plus_data[offset]\n","        else:\n","            expression_label = self.expression_minus_data[offset]\n","\n","        expression_label = torch.tensor(expression_label, dtype=torch.long)\n","\n","        return encoded_sequence, expression_label, region_info.to_dict()\n","\n","\n","# --- Instantiating and trying out the class ---\n","print(\"\\n--- Instantiating and Testing GenomeExpressionDataset ---\")\n","try:\n","    # 1. Instantiate the dataset:\n","    # Pass the 'data_dir' variable which points to your 'data' folder in Google Drive.\n","    my_dataset = GenomeExpressionDataset(data_dir=data_dir)\n","    print(f\"Successfully instantiated GenomeExpressionDataset.\")\n","    print(f\"Total number of samples (regions) in the dataset: {len(my_dataset)}\")\n","\n","    # 2. Access a single sample using __getitem__\n","    # You can access individual samples by their index, like a list.\n","    print(\"\\n--- Accessing individual samples ---\")\n","    sample_index_0 = 0 # Get the first sample\n","    sequence_0, label_0, metadata_0 = my_dataset[sample_index_0]\n","    print(f\"Sample at index {sample_index_0}:\")\n","    print(f\"  Sequence shape: {sequence_0.shape} (One-hot encoded)\")\n","    print(f\"  Label: {label_0.item()}\") # .item() gets the scalar value from a 0-dim tensor\n","    print(f\"  Metadata: {metadata_0}\")\n","\n","    # Get another sample, e.g., at a different index\n","    sample_index_500 = 500 # Get the 501st sample\n","    sequence_500, label_500, metadata_500 = my_dataset[sample_index_500]\n","    print(f\"\\nSample at index {sample_index_500}:\")\n","    print(f\"  Sequence shape: {sequence_500.shape} (One-hot encoded)\")\n","    print(f\"  Label: {label_500.item()}\")\n","    print(f\"  Metadata: {metadata_500}\")\n","\n","    # 3. Use it with PyTorch's DataLoader\n","    # The DataLoader is what typically iterates over your dataset in batches during training.\n","    print(\"\\n--- Using DataLoader to get batches ---\")\n","    batch_size = 8 # Define your desired batch size\n","    # num_workers=0 is usually recommended for debugging in Colab to avoid multiprocessing issues.\n","    # Set to >0 for faster data loading in production.\n","    data_loader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n","\n","    # Iterate through a few batches\n","    for batch_idx, (sequences, labels, metadata) in enumerate(data_loader):\n","        print(f\"\\n--- Batch {batch_idx + 1} ---\")\n","        print(f\"  Batch of sequences shape: {sequences.shape}\") # (batch_size, window_size, num_nucleotides)\n","        print(f\"  Batch of labels shape: {labels.shape}\")       # (batch_size,)\n","        print(f\"  Labels in this batch: {labels.numpy()}\") # .numpy() to see the values easily\n","        print(f\"  Metadata for first sample in batch:\")\n","        print(f\"    Contig: {metadata['contig'][0]}, Strand: {metadata['strand'][0]}, Offset: {metadata['offset'][0]}\")\n","\n","        if batch_idx >= 1: # Print only the first 2 batches for brevity\n","            break\n","\n","except RuntimeError as e:\n","    print(f\"\\nError during dataset instantiation or usage: {e}\")\n","    print(\"Please ensure your Google Drive path is correct and data files are accessible.\")\n","except Exception as e:\n","    print(f\"\\nAn unexpected error occurred: {e}\")"]}]}