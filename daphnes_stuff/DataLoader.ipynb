{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPciPQiKxRgH9mwkR3Eq1A8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "_Aw8LxoFriVv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1749821901568,
     "user_tz": -120,
     "elapsed": 57309,
     "user": {
      "displayName": "Daphn√© Baudeu",
      "userId": "10137981154069023525"
     }
    },
    "outputId": "7d720093-f1af-4dc7-a19e-eec2078fff7a",
    "ExecuteTime": {
     "end_time": "2025-06-21T17:27:59.596067Z",
     "start_time": "2025-06-21T17:27:08.938697Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from Bio import SeqIO # Uncomment if you want to inspect GFF3 files (e.g., ensembl_annotation.gff3) later with Biopython\n",
    "\n",
    "# --- STEP 1: Mount Google Drive ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# --- Configuration ---\n",
    "google_drive_project_path = '/content/gdrive/MyDrive/DnARnAProject/'\n",
    "data_dir = os.path.join(google_drive_project_path, 'data/')\n",
    "\n",
    "print(f\"Attempting to access data in: {data_dir}\")\n",
    "\n",
    "# --- Verify the data directory exists ---\n",
    "if not os.path.isdir(data_dir):\n",
    "    print(f\"Error: The directory '{data_dir}' does not exist.\")\n",
    "    print(\"Please check your Google Drive path and folder names for typos.\")\n",
    "    print(\"Listing contents of your project folder for debugging:\")\n",
    "    try:\n",
    "        print(os.listdir(google_drive_project_path))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Project folder '{google_drive_project_path}' not found either. Check the full path.\")\n",
    "\n",
    "\n",
    "# --- 1. Inspect data.npz ---\n",
    "print(\"--- Inspecting data.npz ---\")\n",
    "try:\n",
    "    data_npz_path = os.path.join(data_dir, 'data.npz')\n",
    "    data_npz = np.load(data_npz_path)\n",
    "\n",
    "    print(f\"Keys in data.npz: {list(data_npz.keys())}\")\n",
    "\n",
    "    if 'sequence' in data_npz:\n",
    "        seq_array = data_npz['sequence']\n",
    "        print(f\"Sequence array shape: {seq_array.shape}\")\n",
    "        print(f\"Sequence array dtype: {seq_array.dtype}\")\n",
    "        print(f\"First 100 sequence values: {seq_array[:100]}\")\n",
    "        unique_seq_values = np.unique(seq_array[:10000])\n",
    "        print(f\"Unique values in sequence (sample): {unique_seq_values}\")\n",
    "        counts = dict(Counter(seq_array))\n",
    "        print(\"Base counts in entire sequence array:\")\n",
    "        print(counts)\n",
    "        print(\"Unique values in first 1000 bases:\", np.unique(seq_array[:1000]))\n",
    "        print(\"Unique values in first 10000 bases:\", np.unique(seq_array[:10000]))\n",
    "\n",
    "\n",
    "    if 'expressed_plus' in data_npz:\n",
    "        expr_plus_array = data_npz['expressed_plus']\n",
    "        print(f\"Expression_plus array shape: {expr_plus_array.shape}\")\n",
    "        print(f\"Expression_plus array dtype: {expr_plus_array.dtype}\")\n",
    "        print(f\"First 10 expression_plus values: {expr_plus_array[:10]}\")\n",
    "        unique_expr_values = np.unique(expr_plus_array[:10000])\n",
    "        print(f\"Unique values in expression_plus (sample): {unique_expr_values}\")\n",
    "\n",
    "    if 'expressed_minus' in data_npz:\n",
    "        expr_minus_array = data_npz['expressed_minus']\n",
    "        print(f\"Expression_minus array shape: {expr_minus_array.shape}\")\n",
    "        print(f\"Expression_minus array dtype: {expr_minus_array.dtype}\")\n",
    "        print(f\"First 10 expression_minus values: {expr_minus_array[:10]}\")\n",
    "        unique_expr_values = np.unique(expr_minus_array[:10000])\n",
    "        print(f\"Unique values in expression_minus (sample): {unique_expr_values}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {data_npz_path} not found. This typically means the path is incorrect after mounting Drive.\")\n",
    "    print(\"Double-check the folder names in your Google Drive for typos.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading data.npz: {e}\")\n",
    "\n",
    "# --- 2. Inspect regions.parquet ---\n",
    "print(\"\\n--- Inspecting regions.parquet ---\")\n",
    "try:\n",
    "    regions_parquet_path = os.path.join(data_dir, 'regions.parquet')\n",
    "    regions_df = pd.read_parquet(regions_parquet_path)\n",
    "\n",
    "    print(f\"Regions DataFrame shape: {regions_df.shape}\")\n",
    "    print(\"Regions DataFrame head:\")\n",
    "    print(regions_df.head())\n",
    "    print(\"\\nRegions DataFrame info:\")\n",
    "    regions_df.info()\n",
    "\n",
    "    print(f\"\\nUnique strands: {regions_df['strand'].unique()}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {regions_parquet_path} not found. This typically means the path is incorrect after mounting Drive.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading regions.parquet: {e}\")\n",
    "\n",
    "\n",
    "# --- 3. Inspect ensembl_annotation.gff3 (Optional) ---\n",
    "print(\"\\n--- Inspecting ensembl_annotation.gff3 (Optional) ---\")\n",
    "try:\n",
    "    gff3_path = os.path.join(data_dir, 'ensembl_annotation.gff3')\n",
    "    with open(gff3_path, \"r\") as handle:\n",
    "        gff_lines = [next(handle) for _ in range(10)]\n",
    "        print(\"First 10 lines of GFF3 file:\")\n",
    "        for line in gff_lines:\n",
    "            print(line.strip())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {gff3_path} not found. This file is optional for the DataLoader.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while inspecting ensembl_annotation.gff3: {e}\")\n",
    "\n",
    "\n",
    "# --- Custom PyTorch DataLoader Class (modified) ---\n",
    "class GenomeExpressionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading DNA sequence and expression data for genomic regions.\n",
    "    It loads data from pre-processed .npz and .parquet files.\n",
    "    Handles reverse complement for '-' strand sequences and reverses expression labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading the full sequence and expression arrays\n",
    "        and the DataFrame of genomic regions.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): The path to the directory containing 'data.npz' and 'regions.parquet'.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.data_npz_path = os.path.join(data_dir, 'data.npz')\n",
    "        self.regions_parquet_path = os.path.join(data_dir, 'regions.parquet')\n",
    "\n",
    "        try:\n",
    "            self.data_npz = np.load(self.data_npz_path)\n",
    "            self.sequence_data = self.data_npz['sequence']\n",
    "            self.expression_plus_data = self.data_npz['expressed_plus']\n",
    "            self.expression_minus_data = self.data_npz['expressed_minus']\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Could not load data from {self.data_npz_path}. Make sure the file exists and is not corrupted: {e}\")\n",
    "\n",
    "        try:\n",
    "            self.regions_df = pd.read_parquet(self.regions_parquet_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Could not load regions from {self.regions_parquet_path}. Make sure the file exists and is not corrupted: {e}\")\n",
    "\n",
    "        self.num_nucleotides = 5 # A, C, G, T, N (mapped to 0, 1, 2, 3, 4)\n",
    "\n",
    "        # Define the complement mapping for integer-encoded bases\n",
    "        # Assuming A=0, C=1, G=2, T=3, N=4\n",
    "        # Complement: A<->T, C<->G, N<->N\n",
    "        # 0<->3, 1<->2, 4<->4\n",
    "        self.complement_map = np.array([3, 2, 1, 0, 4], dtype=np.uint8)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.regions_df)\n",
    "\n",
    "    def _one_hot_encode(self, sequence_segment):\n",
    "        \"\"\"\n",
    "        Converts a sequence segment (array of integer encodings) into a one-hot encoded tensor.\n",
    "        \"\"\"\n",
    "        one_hot_tensor = torch.zeros(len(sequence_segment), self.num_nucleotides, dtype=torch.float32)\n",
    "        one_hot_tensor.scatter_(1, torch.tensor(sequence_segment).unsqueeze(1).long(), 1)\n",
    "        return one_hot_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        region_info = self.regions_df.iloc[idx]\n",
    "\n",
    "        offset = region_info['offset']\n",
    "        window_size = region_info['window_size']\n",
    "        strand = region_info['strand']\n",
    "\n",
    "        # Extract sequence segment\n",
    "        sequence_segment = self.sequence_data[offset : offset + window_size].copy() # .copy() to avoid modifying original array\n",
    "\n",
    "        # Extract expression segment (for the whole window, even if we only use the center label)\n",
    "        # We extract the window for expression data to allow for potential future\n",
    "        # modeling of expression across the window, or just for consistency in handling.\n",
    "        # For now, we only care about the label at `offset`.\n",
    "        expression_segment_plus = self.expression_plus_data[offset : offset + window_size].copy()\n",
    "        expression_segment_minus = self.expression_minus_data[offset : offset + window_size].copy()\n",
    "\n",
    "\n",
    "        if strand == '+':\n",
    "            # For the forward strand, sequence and expression are used as is.\n",
    "            encoded_sequence = self._one_hot_encode(sequence_segment)\n",
    "            expression_label = expression_segment_plus[0] # Use label at the start of the window\n",
    "        else: # strand == '-'\n",
    "            # 1. Reverse complement the sequence\n",
    "            # First, complement the bases, then reverse the order\n",
    "            reverse_complemented_sequence = self.complement_map[sequence_segment][::-1].copy()\n",
    "            encoded_sequence = self._one_hot_encode(reverse_complemented_sequence)\n",
    "\n",
    "            # 2. Reverse the expression segment and take the appropriate label\n",
    "            # If the sequence is reversed, the corresponding expression values\n",
    "            # should also be considered in reverse order to maintain alignment.\n",
    "            # The label for the '-' strand region is associated with the 'offset'\n",
    "            # when read from the original chromosome (forward strand coordinates).\n",
    "            # When we flip the sequence, the *first* base of the reverse complement\n",
    "            # corresponds to the *last* base of the original segment.\n",
    "            # So, if expression_segment_minus was [val_at_offset, ..., val_at_offset+window_size-1],\n",
    "            # and we reverse it, the label would be the last element of the original segment.\n",
    "            # However, the problem statement often means that the label for the region\n",
    "            # (which is defined by its start 'offset') should be taken from the\n",
    "            # 'expressed_minus' array at that *same* offset.\n",
    "            # Let's stick to using the `offset` index for the label as it's defined\n",
    "            # in your `regions.parquet` and the original implementation.\n",
    "            expression_label = expression_segment_minus[0] # Use label at the start of the window from the minus strand data\n",
    "\n",
    "\n",
    "        expression_label = torch.tensor(expression_label, dtype=torch.long)\n",
    "\n",
    "        return encoded_sequence, expression_label, region_info.to_dict()\n",
    "\n",
    "\n",
    "# --- Instantiating and trying out the class ---\n",
    "print(\"\\n--- Instantiating and Testing GenomeExpressionDataset ---\")\n",
    "try:\n",
    "    my_dataset = GenomeExpressionDataset(data_dir=data_dir)\n",
    "    print(f\"Successfully instantiated GenomeExpressionDataset.\")\n",
    "    print(f\"Total number of samples (regions) in the dataset: {len(my_dataset)}\")\n",
    "\n",
    "    print(\"\\n--- Accessing individual samples ---\")\n",
    "    # Test a sample from the '+' strand\n",
    "    sample_plus_idx = regions_df[regions_df['strand'] == '+'].index[0]\n",
    "    sequence_plus, label_plus, metadata_plus = my_dataset[sample_plus_idx]\n",
    "    print(f\"Sample at index {sample_plus_idx} (strand +):\")\n",
    "    print(f\"  Sequence shape: {sequence_plus.shape} (One-hot encoded)\")\n",
    "    print(f\"  Label: {label_plus.item()}\")\n",
    "    print(f\"  Metadata: {metadata_plus}\")\n",
    "\n",
    "    # Test a sample from the '-' strand\n",
    "    sample_minus_idx = regions_df[regions_df['strand'] == '-'].index[0]\n",
    "    sequence_minus, label_minus, metadata_minus = my_dataset[sample_minus_idx]\n",
    "    print(f\"\\nSample at index {sample_minus_idx} (strand -):\")\n",
    "    print(f\"  Sequence shape: {sequence_minus.shape} (One-hot encoded)\")\n",
    "    print(f\"  Label: {label_minus.item()}\")\n",
    "    print(f\"  Metadata: {metadata_minus}\")\n",
    "\n",
    "    # You can add checks here to confirm reverse complement if you want,\n",
    "    # e.g., by converting sequence_plus back to integer array and comparing\n",
    "    # its reverse complement to the integer array of sequence_minus (if they were designed to be complements).\n",
    "\n",
    "    # 3. Use it with PyTorch's DataLoader\n",
    "    print(\"\\n--- Using DataLoader to get batches ---\")\n",
    "    batch_size = 8\n",
    "    data_loader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    for batch_idx, (sequences, labels, metadata) in enumerate(data_loader):\n",
    "        print(f\"\\n--- Batch {batch_idx + 1} ---\")\n",
    "        print(f\"  Batch of sequences shape: {sequences.shape}\")\n",
    "        print(f\"  Batch of labels shape: {labels.shape}\")\n",
    "        print(f\"  Labels in this batch: {labels.numpy()}\")\n",
    "        print(f\"  Metadata for first sample in batch:\")\n",
    "        print(f\"    Contig: {metadata['contig'][0]}, Strand: {metadata['strand'][0]}, Offset: {metadata['offset'][0]}\")\n",
    "\n",
    "        if batch_idx >= 1:\n",
    "            break\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nError during dataset instantiation or usage: {e}\")\n",
    "    print(\"Please ensure your Google Drive path is correct and data files are accessible.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 10\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset, DataLoader\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# from Bio import SeqIO # Uncomment if you want to inspect GFF3 files (e.g., ensembl_annotation.gff3) later with Biopython\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# --- STEP 1: Mount Google Drive ---\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m drive\n\u001B[0;32m     11\u001B[0m drive\u001B[38;5;241m.\u001B[39mmount(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/gdrive\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# --- Configuration ---\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "execution_count": 1
  }
 ]
}
